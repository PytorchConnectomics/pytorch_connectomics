┌─────────────────────────────────────────────────────────────────────────────┐
│                   PyTorch Connectomics + nnUNet Integration                 │
│                          Large-Scale Inference Pipeline                     │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                                INPUT LAYER                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   ┌─────────────┐ │
│  │  File List   │   │ Glob Pattern │   │ TensorStore  │   │  Cloud S3   │ │
│  │   (*.h5)     │   │  (*.tiff)    │   │   (Zarr)     │   │  (future)   │ │
│  └──────┬───────┘   └──────┬───────┘   └──────┬───────┘   └──────┬──────┘ │
│         │                  │                  │                  │         │
│         └──────────────────┴──────────────────┴──────────────────┘         │
│                                      │                                      │
└──────────────────────────────────────┼──────────────────────────────────────┘
                                       │
                                       ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           CONFIGURATION LAYER                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  tutorials/nnunet_mito_inference.yaml (Hydra Config)               │    │
│  ├────────────────────────────────────────────────────────────────────┤    │
│  │  model:                                                             │    │
│  │    architecture: nnunet                                             │    │
│  │    nnunet_checkpoint: /path/to/mito_semantic_2d.pth                │    │
│  │                                                                      │    │
│  │  inference:                                                          │    │
│  │    volume_mode:                                                      │    │
│  │      enabled: true                                                   │    │
│  │      file_pattern: "/data/*.h5"                                     │    │
│  │    sliding_window:                                                   │    │
│  │      window_size: [384, 512]                                        │    │
│  │      overlap: 0.5                                                    │    │
│  │    test_time_augmentation:                                           │    │
│  │      flip_axes: [[1], [2], [1,2]]                                   │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└──────────────────────────────────────┼──────────────────────────────────────┘
                                       │
                                       ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                          MODEL LOADING LAYER                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  nnUNetWrapper (connectomics/models/arch/nnunet_models.py)         │    │
│  ├────────────────────────────────────────────────────────────────────┤    │
│  │                                                                      │    │
│  │  1. Load checkpoint.pth ──────────────────────┐                    │    │
│  │  2. Load plans.json (architecture params) ─────┤                   │    │
│  │  3. Load dataset.json (labels, channels) ──────┤                   │    │
│  │                                                 │                    │    │
│  │  4. Build nnUNet network (Dynamic UNet) ◀──────┘                   │    │
│  │  5. Load state_dict weights                                         │    │
│  │                                                                      │    │
│  │  forward(x: Tensor) -> Tensor                                       │    │
│  │    • Input: (B, C, H, W) for 2D or (B, C, D, H, W) for 3D         │    │
│  │    • Output: (B, num_classes, H, W) or (B, num_classes, D, H, W)  │    │
│  │                                                                      │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└──────────────────────────────────────┼──────────────────────────────────────┘
                                       │
                                       ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                     VOLUME PROCESSING LAYER (NEW)                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  VolumeProcessor (connectomics/lightning/inference.py)             │    │
│  ├────────────────────────────────────────────────────────────────────┤    │
│  │                                                                      │    │
│  │  FOR each file in file_list:                                        │    │
│  │    ┌──────────────────────────────────────────────────────┐       │    │
│  │    │ 1. Check if output exists → SKIP if exists           │       │    │
│  │    │ 2. Load volume from disk (HDF5/TIFF/PNG)            │       │    │
│  │    │ 3. Convert to tensor: (1, C, D, H, W)               │       │    │
│  │    │ 4. Pass to InferenceManager ──────────────────────┐ │       │    │
│  │    └──────────────────────────────────────────────────┼─┘       │    │
│  │                                                         │          │    │
│  │  Progress: [████████░░] 427/1000 (42.7%)              │          │    │
│  │  Skipped: 273 existing files                            │          │    │
│  │  ETA: 2.5 hours                                         │          │    │
│  │                                                         │          │    │
│  └─────────────────────────────────────────────────────────┼──────────┘    │
│                                                            │               │
└────────────────────────────────────────────────────────────┼───────────────┘
                                                             │
                                                             ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                        INFERENCE MANAGER LAYER                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  InferenceManager (connectomics/lightning/inference.py)            │    │
│  ├────────────────────────────────────────────────────────────────────┤    │
│  │                                                                      │    │
│  │  ┌─────────────────────────────────────────────────────────────┐  │    │
│  │  │  SLIDING WINDOW INFERENCE (MONAI)                            │  │    │
│  │  ├─────────────────────────────────────────────────────────────┤  │    │
│  │  │  Input: Large volume (1, C, 512, 512) or (1, C, D, H, W)   │  │    │
│  │  │  ROI Size: [384, 512] (from nnUNet plans)                   │  │    │
│  │  │  Overlap: 0.5 (50% overlap between tiles)                   │  │    │
│  │  │  Blending: Gaussian (smooth tile boundaries)                │  │    │
│  │  │                                                              │  │    │
│  │  │  ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐                   │  │    │
│  │  │  │Tile 1│  │Tile 2│  │Tile 3│  │Tile 4│  ...              │  │    │
│  │  │  └───┬──┘  └───┬──┘  └───┬──┘  └───┬──┘                   │  │    │
│  │  │      │         │         │         │                        │  │    │
│  │  │      └─────────┴─────────┴─────────┘                       │  │    │
│  │  │                      │                                      │  │    │
│  │  │           Batch tiles → nnUNet model                       │  │    │
│  │  │                      │                                      │  │    │
│  │  │           Blend predictions with Gaussian weights          │  │    │
│  │  │                      │                                      │  │    │
│  │  │  Output: Full volume predictions (1, num_classes, H, W)    │  │    │
│  │  └──────────────────────┼──────────────────────────────────────┘  │    │
│  │                         │                                          │    │
│  │                         ▼                                          │    │
│  │  ┌─────────────────────────────────────────────────────────────┐  │    │
│  │  │  TEST-TIME AUGMENTATION (TTA)                               │  │    │
│  │  ├─────────────────────────────────────────────────────────────┤  │    │
│  │  │  Augmentation variants:                                      │  │    │
│  │  │    • Original (no flip)                                      │  │    │
│  │  │    • Horizontal flip (axis=1)                                │  │    │
│  │  │    • Vertical flip (axis=2)                                  │  │    │
│  │  │    • Both flips (axis=[1,2])                                │  │    │
│  │  │                                                              │  │    │
│  │  │  For each variant:                                           │  │    │
│  │  │    1. Apply flip to input                                    │  │    │
│  │  │    2. Run sliding window inference                           │  │    │
│  │  │    3. Reverse flip on output                                 │  │    │
│  │  │    4. Apply softmax activation                               │  │    │
│  │  │                                                              │  │    │
│  │  │  Ensemble: Running average (memory-efficient)                │  │    │
│  │  │    result = mean([pred1, pred2, pred3, pred4])              │  │    │
│  │  │                                                              │  │    │
│  │  │  Output: Ensemble predictions (1, num_classes, H, W)        │  │    │
│  │  └──────────────────────┼──────────────────────────────────────┘  │    │
│  └────────────────────────┼───────────────────────────────────────────┘    │
│                           │                                                │
└───────────────────────────┼────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                       POST-PROCESSING LAYER                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  apply_postprocessing() (connectomics/lightning/inference.py)      │    │
│  ├────────────────────────────────────────────────────────────────────┤    │
│  │                                                                      │    │
│  │  Step 1: Binary Operations                                          │    │
│  │    • Threshold: prob > 0.5                                          │    │
│  │    • Remove small objects (< 100 px)                                │    │
│  │    • Fill holes                                                      │    │
│  │                                                                      │    │
│  │  Step 2: Scaling                                                     │    │
│  │    • Multiply by 255.0 (for uint8 output)                           │    │
│  │                                                                      │    │
│  │  Step 3: Dtype Conversion                                            │    │
│  │    • Convert to uint8 with clamping                                 │    │
│  │                                                                      │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                      │                                      │
│                                      ▼                                      │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  apply_decode_mode() (connectomics/decoding/segmentation.py)       │    │
│  ├────────────────────────────────────────────────────────────────────┤    │
│  │                                                                      │    │
│  │  Instance Segmentation (optional):                                  │    │
│  │    • decode_binary_watershed: Watershed transform                  │    │
│  │    • decode_binary_cc: Connected components                        │    │
│  │    • decode_binary_contour_watershed: Advanced watershed           │    │
│  │                                                                      │    │
│  │  Output: Instance mask (each object has unique ID)                  │    │
│  │                                                                      │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└──────────────────────────────────────┼──────────────────────────────────────┘
                                       │
                                       ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                            OUTPUT LAYER                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  write_outputs() (connectomics/lightning/inference.py)             │    │
│  ├────────────────────────────────────────────────────────────────────┤    │
│  │                                                                      │    │
│  │  For each sample in batch:                                          │    │
│  │    • Extract filename from metadata                                 │    │
│  │    • Apply output transpose (if configured)                         │    │
│  │    • Select channels to save (save_channels)                        │    │
│  │    • Write to HDF5/TIFF                                             │    │
│  │                                                                      │    │
│  │  Output format:                                                      │    │
│  │    /output/path/volume_001_prediction.h5                            │    │
│  │    /output/path/volume_002_prediction.h5                            │    │
│  │    ...                                                               │    │
│  │                                                                      │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└──────────────────────────────────────┼──────────────────────────────────────┘
                                       │
                                       ▼
                              ┌─────────────────┐
                              │  Saved Results  │
                              │   (HDF5/TIFF)   │
                              └─────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                       DISTRIBUTED INFERENCE (OPTIONAL)                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  GPU 0: Process files 0, 4, 8, 12, ...  (step=4, start=0)                  │
│  GPU 1: Process files 1, 5, 9, 13, ...  (step=4, start=1)                  │
│  GPU 2: Process files 2, 6, 10, 14, ... (step=4, start=2)                  │
│  GPU 3: Process files 3, 7, 11, 15, ... (step=4, start=3)                  │
│                                                                              │
│  No synchronization needed - each GPU works independently!                  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                              KEY BENEFITS                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ✅ Zero-Copy Model Loading: No temporary file overhead                     │
│  ✅ Memory-Efficient: Process volumes one at a time                         │
│  ✅ Scalable: Linear speedup with multiple GPUs                             │
│  ✅ Resumable: Skip existing outputs automatically                          │
│  ✅ Format-Agnostic: HDF5, TIFF, PNG, Zarr support                          │
│  ✅ Production-Ready: Error recovery, monitoring, logging                   │
│  ✅ Modern Stack: Lightning + MONAI + Hydra                                 │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
