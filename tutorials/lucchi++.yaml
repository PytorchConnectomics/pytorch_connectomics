# Lucchi++ Mitochondria Segmentation
# Electron microscopy (EM) dataset with multiple architecture options
#
# ============================================================================
# ARCHITECTURE SELECTION - Change 'architecture' to switch models:
# ============================================================================
#
# monai_unet (recommended for MONAI)
#   - MONAI's UNet with residual units
#   - Supports any number of levels (uses filters directly)
#   - No deep supervision
#   - Recommended for: MONAI baseline, flexible architecture
#
# monai_basic_unet3d
#   - MONAI's BasicUNet (always 6 levels, pads filters if < 6)
#   - Simple, fast
#   - Recommended for: Quick experiments
#
# rsunet
#   - Residual Symmetric U-Net (EM-optimized)
#   - No checkerboard artifacts (uses upsample+conv instead of transposed conv)
#   - Anisotropic convolutions for EM data
#   - Recommended for: Production EM segmentation, anisotropic data
#
# mednext
#   - MedNeXt (MICCAI 2023, ConvNeXt-based)
#   - State-of-the-art performance
#   - Deep supervision for better training
#   - Sizes: S (5.6M), B (10.5M), M (17.6M), L (61.8M) params
#   - Recommended for: Best accuracy, sufficient GPU memory
#
# ============================================================================

experiment_name: lucchi++
description: Mitochondria segmentation on Lucchi++ EM dataset

# System
system:
  training:
    num_gpus: 4
    num_cpus: 8
    num_workers: 8           # Set to 0 to avoid /dev/shm space issues (use in-process loading)
    batch_size: 1
  inference:
    num_gpus: 1
    num_cpus: 1
    num_workers: 1           # Set to 0 to avoid /dev/shm space issues
    batch_size: 1            # Reduced from 16 to avoid OOM (sw_batch_size will use this)
  seed: 42

# Model Configuration
model:
  # ========== CHANGE THIS LINE TO SWITCH ARCHITECTURES ==========
  architecture: monai_unet  # Options: monai_unet, monai_basic_unet3d, rsunet, mednext
  # ==============================================================

  # Common settings (used by all architectures)
  input_size: [112, 112, 112]
  output_size: [112, 112, 112]
  in_channels: 1
  out_channels: 1                      # Single channel with BCE loss (standard for EM)
  filters: [32, 64, 128, 256]          # 4-level encoder
  dropout: 0.1

  # MONAI BasicUNet-specific settings (ignored by other architectures)
  upsample: nontrainable                # Use trilinear upsampling (no transposed conv)

  # RSUNet-specific settings (ignored by other architectures)
  rsunet_norm: batch                    # Batch normalization for RSUNet

  # MedNeXt-specific settings (ignored by other architectures)
  mednext_size: S                       # S (5.6M), B (10.5M), M (17.6M), L (61.8M)
  mednext_kernel_size: 3                # 3, 5, or 7
  deep_supervision: false                # Enable deep supervision for MedNeXt

  # Loss configuration - WeightedBCE + Dice for mitochondria segmentation
  loss_functions: [WeightedBCE, DiceLoss]
  loss_weights: [1.0, 1.0]             # Equal weighting for BCE and Dice
  loss_kwargs:
    - {reduction: mean}                            # WeightedBCE: average over batch
    - {include_background: false, sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}  # DiceLoss with sigmoid

# Data - Using automatic 80/20 train/val split (DeepEM-style)
data:
  # Volume configuration
  train_image: datasets/lucchi++/train_im.h5
  train_label: datasets/lucchi++/train_mito.h5
  train_resolution: [5, 5, 5]        # Lucchi EM: 5nm isotropic resolution
  use_preloaded_cache: true            # Load volumes into memory for fast training

  # Patch configuration
  patch_size: [112, 112, 112]           # Isotropic patches for training
  pad_size: [0, 0, 0]                   # No padding during training (not needed)
  iter_num_per_epoch: 1280              # 1280 random crops per epoch

  # Image normalization
  image_transform:
    normalize: "0-1"                   # Min-max normalization to [0, 1]
    clip_percentile_low: 0.0           # No clipping
    clip_percentile_high: 1.0

  # Augmentation - moderate set for 3D mitochondria segmentation
  # Recommended for Lucchi++: geometric transforms + EM-specific augmentations
  augmentation:
    preset: "some"  # Enable only augmentations explicitly set to enabled=True

    # Standard geometric augmentations (safe for 3D EM)
    flip:
      enabled: true
      prob: 0.5
      spatial_axis: [0, 1, 2]  # Flip x/y/z

    rotate:
      enabled: true
      prob: 0.5
      spatial_axes: [0, 1, 2]  # Rotate x/y/z

    affine:
      enabled: true
      prob: 0.3  # Lower prob to avoid too aggressive transforms
      rotate_range: [0.1, 0.1, 0.1]  # Small rotations (~6°) - careful with Z-axis
      scale_range: [0.05, 0.05, 0.05]  # Small scaling (±5%)
      shear_range: [0.05, 0.05, 0.05]  # Small shearing

    # Intensity augmentations (important for EM data variability)
    intensity:
      enabled: true
      gaussian_noise_prob: 0.2  # Moderate noise
      gaussian_noise_std: 0.03
      shift_intensity_prob: 0.4
      shift_intensity_offset: 0.1
      contrast_prob: 0.4
      contrast_range: [0.8, 1.2]  # Moderate contrast variation

    # EM-specific augmentations (highly recommended for EM data)
    misalignment:
      enabled: true
      prob: 0.4
      displacement: 8  # Moderate displacement for small patches
      rotate_ratio: 0.3  # Mix of translation and rotation

    missing_section:
      enabled: true
      prob: 0.3
      num_sections: 2  # 1-2 missing sections (common in EM)

    motion_blur:
      enabled: true
      prob: 0.3
      sections: 2
      kernel_size: 9  # Moderate blur

    # Avoid these for mitochondria segmentation:
    # - elastic: Too aggressive for small structures
    # - cut_blur/cut_noise: May be too aggressive
    # - copy_paste/mixup: Not needed for binary segmentation


# Optimizer - Adam with conservative hyperparameters (proven for EM segmentation)
optimization:
  max_epochs: 1000                      # Standard epochs for EM segmentation
  gradient_clip_val: 0.5               # Conservative gradient clipping
  accumulate_grad_batches: 1
  precision: "bf16-mixed"              # BFloat16 mixed precision

  optimizer:
    name: Adam                         # Standard Adam (not AdamW for EM tasks)
    lr: 0.001                          # Learning rate (1e-3 works well for all architectures)
    weight_decay: 0.0                  # No weight decay (not beneficial for EM)
    betas: [0.9, 0.999]                # Standard Adam betas
    eps: 1.0e-8                        # Numerical stability

  # Scheduler - ReduceLROnPlateau for adaptive learning
  scheduler:
    name: ReduceLROnPlateau           # Reduce LR when validation loss plateaus
    mode: min                         # Monitor minimum loss
    factor: 0.5                       # Reduce LR by 50%
    patience: 50                      # Wait 50 epochs before reducing
    threshold: 1.0e-4                 # Minimum change to qualify as improvement
    min_lr: 1.0e-6                    # Don't go below 1e-6

monitor:
  # Loss monitoring and validation frequency
  detect_anomaly: false
  logging:
    # scalar loss
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 10
      val_check_interval: 1.0
      benchmark: true

    # visualization
    images:
      enabled: true
      max_images: 8
      num_slices: 2
      log_every_n_epochs: 1                # Log every N epochs (default: 1)
      channel_mode: argmax                 # 'argmax', 'all', or 'selected'
      selected_channels: null              # Only used when channel_mode='selected'

  # Checkpointing
  checkpoint:
    mode: min
    save_top_k: 1
    save_last: true
    save_every_n_epochs: 10
    # checkpoint_filename: auto-generated from monitor metric (epoch={epoch:03d}-{monitor}={value:.4f})
    use_timestamp: true       # Enable timestamped subdirectories (YYYYMMDD_HHMMSS)

  # Early stopping - Patient for convergence
  early_stopping:
    enabled: true
    monitor: train_loss_total_epoch
    patience: 100        # Patient waiting for improvement
    mode: min
    min_delta: 1.0e-4    # Minimum delta for improvement
    check_finite: true   # Stop if monitored metric becomes NaN/inf
    threshold: 0.02      # Stop if loss gets this low (excellent convergence for EM)
    divergence_threshold: 2.0  # Stop if loss exceeds this (training collapse)

# Inference - MONAI SlidingWindowInferer
inference:
  data:
    test_image: datasets/lucchi++/test_im.h5
    test_label: datasets/lucchi++/test_mito.h5
    test_resolution: [5, 5, 5]

  # MONAI SlidingWindowInferer parameters
  sliding_window:
    window_size: [112, 112, 112]          # Patch size (matches training patches)
    sw_batch_size: 1                      # Process 1 patch at a time (memory optimization)
    overlap: 0.25                        # 25% overlap (reduced from 0.5 to save memory)
    blending: gaussian                    # Gaussian weighting for smooth blending
    sigma_scale: 0.25                     # Larger sigma = smoother blending at boundaries
    padding_mode: replicate               # Replicate edge values (better than reflect for z=0)

  # Test-Time Augmentation (TTA)
  test_time_augmentation:
    enabled: true        # Enable TTA for improved predictions
    flip_axes: null      # No flip augmentation (set to null to disable flips)
    # XY flips are safe for isotropic XY resolution, Z-flip avoided due to anisotropy
    channel_activations: [[0, 1, 'sigmoid']]  # Sigmoid activation for single-channel output
    select_channel: null                   # Keep all channels (single channel output)
    ensemble_mode: mean              # Mean ensemble (smooth predictions)
    # NOTE: Reduced TTA compared to original (4x instead of 8x) for faster inference


  # Postprocessing configuration (applied AFTER TTA if enabled)
  postprocessing:
    intensity_scale: 255                    # Scale predictions to [0, 255] for saving
    intensity_dtype: uint8                  # Save as uint8

  # Evaluation
  evaluation:
    enabled: true                        # Use eval mode for BatchNorm
    metrics: [jaccard]             # Metrics to compute

  # NOTE: batch_size=1 for inference
  #   During training: batch_size controls how many random patches to load
  #   During inference: batch_size=1 means process one full volume at a time
  #   sw_batch_size (above) controls how many patches are processed per GPU forward pass
