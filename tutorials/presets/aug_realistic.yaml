# Realistic EM Augmentation Preset (BANIS-Style)
# Use case: Replicating BANIS augmentation strategy with better implementations
# Training speed: Medium
# Model robustness: Good (realistic EM artifacts)

system:
  training:
    num_gpus: 1
    batch_size: 2
    num_workers: 4
  seed: 42

model:
  architecture: monai_basic_unet3d
  in_channels: 1
  out_channels: 2
  filters: [32, 64, 128, 256, 512]
  dropout: 0.1

  losses:
    - function: DiceLoss
      weight: 1.0
      pred_slice: [0, '${model.out_channels}']
      target_slice: [0, '${model.out_channels}']
    - function: BCEWithLogitsLoss
      weight: 1.0
      pred_slice: [0, '${model.out_channels}']
      target_slice: [0, '${model.out_channels}']

data:
  # Data paths (modify for your dataset)
  train_image: "datasets/lucchi/train_image.h5"
  train_label: "datasets/lucchi/train_label.h5"
  val_image: "datasets/lucchi/val_image.h5"
  val_label: "datasets/lucchi/val_label.h5"

  patch_size: [128, 128, 128]

  # Realistic EM augmentation (BANIS-style but better)
  augmentation:
    preset: some
    # Missing sections (like BANIS DropSliced but better)
    # PyTC actually removes sections vs BANIS just zeros them
    missing_section:
      enabled: true
      prob: 0.5
      num_sections: 2
    # Misalignment (like BANIS ShiftSliced but better)
    # PyTC uses proper geometric transforms vs BANIS circular shifts
    misalignment:
      enabled: true
      prob: 0.5
      displacement: 10  # BANIS uses max_shift=10
      rotate_ratio: 0.0  # Pure translation like BANIS
    # Motion blur (BANIS doesn't have this)
    motion_blur:
      enabled: true
      prob: 0.5
      sections: 2
      kernel_size: 11
    # Standard intensity augmentation (like BANIS)
    intensity:
      enabled: true
      shift_intensity_prob: 0.5
      shift_intensity_offset: 0.1
      contrast_prob: 0.5
      contrast_range: [0.9, 1.1]
      gaussian_noise_prob: 0.5
      gaussian_noise_std: 0.1

optimization:
  max_epochs: 100
  precision: "16-mixed"
  gradient_clip_val: 1.0
  log_every_n_steps: 50
  optimizer:
    name: AdamW
    lr: 1e-4
    weight_decay: 1e-4
  scheduler:
    name: CosineAnnealingLR
    warmup_epochs: 5

monitor:
  checkpoint:
    monitor: "val/loss"
    mode: "min"
    save_top_k: 3
    save_last: true
