# CellMap Challenge - Phase 2: Combination Approach (11 Instance)
#
# PHASE 2: Full submission - Combination approach
# Part 1 of 2: See cellmap_semantic_full.yaml (36 semantic classes)
# Part 2 of 2: 11 instance classes (multi-class binary mask + SDT)
#
# This config trains 11 instance classes (evaluated using Hausdorff Distance + Accuracy).
#
# Instance classes (require SDT post-processing for instance IDs):
# - mito (mitochondria) - CRITICAL: appears in 14/16 test crops
# - endo (endosomes)
# - ld (lipid droplets)
# - lyso (lysosomes)
# - np (nuclear pore)
# - ves (vesicles)
# - peroxisome
# - mt (microtubules)
# - cell (whole cell)
# - nuc (nucleus instances)
# - vim (vimentin)
#
# Strategy:
# - Higher resolution (4nm) for better boundary detection
# - Multi-class binary mask output (11 channels)
# - SDT (Signed Distance Transform) for instance separation
# - Post-processing: Binary mask → SDT → Watershed → Instance IDs
# - Large model (MedNeXt-L) for 11-class capacity
# - Extended training (1500 epochs) for instance quality
# - Deep supervision for multi-scale learning
#
# Usage:
#   python scripts/cellmap/train_cellmap.py --config tutorials/cellmap_instance_full.yaml

experiment_name: cellmap_instance_full_mednext_l
description: CellMap full instance segmentation (11 classes) with MedNeXt-L

# System
system:
  training:
    num_gpus: 4                           # Multi-GPU recommended
    num_cpus: 16
    num_workers: 8
    batch_size: 1                         # Per GPU (effective = 4)
  inference:
    num_gpus: 1
    num_cpus: 4
    num_workers: 2
    batch_size: 1
  seed: 42

# Model Configuration
model:
  architecture: mednext                   # MedNeXt (SOTA for instance segmentation)

  # Input/output configuration
  input_size: [96, 96, 96]                # Higher res patches for boundaries
  output_size: [96, 96, 96]
  in_channels: 1                          # Grayscale EM
  out_channels: 11                        # 11 instance classes

  # MedNeXt configuration (large model for 11 classes)
  mednext_size: L                         # L (61.8M params) - best for multi-class
  mednext_kernel_size: 7                  # 7x7x7 kernels for better context
  deep_supervision: true                  # Multi-scale loss (CRITICAL)

  # Loss configuration (multi-class instance segmentation)
  loss_functions: [DiceLoss, BCEWithLogitsLoss]
  loss_weights: [1.0, 1.0]
  loss_kwargs:
    - {sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}  # Dice for each class
    - {reduction: mean}                                   # BCE for boundaries

# Data - CellMap Challenge Dataset
data:
  # Dataset type (custom for CellMap)
  dataset_type: cellmap

  # CellMap-specific configuration
  cellmap:
    # Data paths
    data_root: /projects/weilab/dataset/cellmap
    datasplit_path: tutorials/cellmap_instance_full_datasplit.csv  # Auto-generated

    # All 11 instance segmentation classes
    classes: [mito, endo, ld, lyso, np, ves, peroxisome, mt, cell, nuc, vim]
    force_all_classes: false              # Don't require all classes (some are rare)

    # Patch configuration (higher resolution for boundaries)
    input_array_info:
      shape: [96, 96, 96]
      scale: [4, 4, 4]                    # 4nm isotropic - best for instance boundaries
    target_array_info:
      shape: [96, 96, 96]
      scale: [4, 4, 4]

    # Spatial augmentation (aggressive for instance segmentation)
    spatial_transforms:
      mirror:
        axes: {x: 0.5, y: 0.5, z: 0.5}    # 50% flip probability per axis
      transpose:
        axes: [x, y, z]                    # Random permutation
      rotate:
        axes:
          x: [-180, 180]
          y: [-180, 180]
          z: [-180, 180]

  # Training configuration
  iter_num_per_epoch: 2000                # Steps per epoch
  persistent_workers: true

# Optimizer - AdamW with lower LR for large model
optimization:
  max_epochs: 1500                        # Extended training for quality
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4              # Effective batch = 4 GPUs * 1 * 4 = 16
  precision: "16-mixed"                   # Mixed precision

  optimizer:
    name: AdamW
    lr: 5e-4                              # Lower LR for large model
    weight_decay: 1e-4

  # Scheduler - cosine annealing with warmup
  scheduler:
    name: CosineAnnealingLR
    warmup_epochs: 10
    min_lr: 1e-6

# Monitoring
monitor:
  detect_anomaly: false

  logging:
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 50
      val_check_interval: 1.0
      benchmark: true

    images:
      enabled: true
      max_images: 2
      num_slices: 4
      log_every_n_epochs: 20              # Less frequent for long training
      channel_mode: all

  # Checkpointing
  checkpoint:
    mode: min
    save_top_k: 5                         # Keep more checkpoints
    save_last: true
    save_every_n_epochs: 100              # Save less frequently
    dirpath: outputs/cellmap_instance_full/checkpoints/
    use_timestamp: true

  # Early stopping (very patient for long training)
  early_stopping:
    enabled: true
    monitor: train_loss_total_epoch
    patience: 200                         # Patient for 1500 epoch training
    mode: min
    min_delta: 1e-6

# Inference configuration (optimized for instance boundaries)
inference:
  sliding_window:
    window_size: [96, 96, 96]
    sw_batch_size: 4                      # Larger for faster inference
    overlap: 0.75                         # High overlap for instance boundaries
    blending: gaussian
    sigma_scale: 0.25
    padding_mode: replicate

  test_time_augmentation:
    flip_axes: [[2], [3], [4]]            # All 3 axes
    rotation90_axes: null                 # Skip rotation for speed
    select_channel: all
    channel_activations:
      - [0, 11, sigmoid]                  # Sigmoid for multi-label
    ensemble_mode: mean
    apply_mask: false

  save_prediction:
    enabled: true
    intensity_scale: -1                   # Scale to [0, 255]

# Test configuration
test:
  data:
    test_image: null                      # Set when running test mode
    test_label: null
    test_mask: null
    test_resolution: [4, 4, 4]

  evaluation:
    enabled: false                        # Enable when test labels available
    metrics: []

# Post-processing (SDT-based instance segmentation)
# Note: Post-processing is done separately using connectomics.decoding
# 1. Binary mask prediction (11 channels, sigmoid output)
# 2. SDT (Signed Distance Transform) computation
# 3. Watershed segmentation on SDT
# 4. Instance ID assignment per class
