# MedNext model for mitochondria segmentation
#
# This config loads a pretrained BANIS (Baseline for Affinity-based Neuron Instance Segmentation)
# MedNeXt model for mitochondria segmentation inference.
#
# The model predicts 7 channels:
#   - Channels 0-2: Short-range affinities (x, y, z directions)
#   - Channels 3-5: Long-range affinities (offset=10 voxels in x, y, z)
#   - Channel 6: Signed Distance Transform (SDT)
#
# Affinity-based segmentation workflow:
#   1. Predict affinities at each voxel
#   2. Apply sigmoid to get probabilities
#   3. Threshold short-range affinities
#   4. Run connected components to get instance segmentation
#
# Training details:
#   - Model: MedNeXt-S with 3x3x3 kernels
#   - Dataset: betaSeg+han24+Jurkat+Cardiac+Kidney+Liver+Sperm+Macrophage
#   - Training: 110K steps, AdamW lr=0.001, CosineAnnealing
#   - Patch size: 128x128x128
#   - Resolution: Variable (16nm isotropic typical)
#
# Usage:
#   # Load external weights via CLI with --external-prefix
#   just test mednext mitoverse /path/to/checkpoint.ckpt --external-prefix model.
#
#   # Or with full command:
#   python scripts/main.py --config tutorials/mednext_mitoverse.yaml --mode test \
#     --checkpoint /path/to/checkpoint.ckpt --external-prefix model.
#
#   # Update inference.data paths before running

experiment_name: mednext_mitoverse
description: MedNeXt-S model for mitochondria segmentation (BANIS affinity prediction)

# System configuration
system:
  inference:
    num_gpus: 1
    num_cpus: 1
    num_workers: 1
    batch_size: 1
  seed: 42

# Model configuration - MedNeXt-S matching BANIS training
model:
  architecture: mednext
  input_size: [128, 128, 128]
  output_size: [128, 128, 128]
  in_channels: 1
  out_channels: 7                    # 6 affinities + 1 SDT

  # MedNeXt architecture (matches BANIS training)
  mednext_size: S                    # Small: 5.6M parameters
  mednext_kernel_size: 3             # 3x3x3 kernels
  mednext_dim: "3d"                  # 3D convolutions
  deep_supervision: false            # BANIS doesn't use deep supervision

  # External model weights can be loaded via CLI:
  #   just test mednext mitoverse /path/to/checkpoint.ckpt --external-prefix model.
  # This will strip the "model." prefix from state_dict keys

  # Loss configuration (for reference, not used in inference)
  # BANIS uses BCEWithLogitsLoss for affinities + MSE for SDT
  loss_functions: [BCEWithLogitsLoss]
  loss_weights: [1.0]

# Inference configuration
inference:
  data:
    # IMPORTANT: Update these paths to your test data
    test_path: "/projects/weilab/liupeng/nnUNet/DATASET/nnUNet_raw/Dataset001_ME2-Beta/"   # Update this path
    test_image: "imagesTs/high_c2_0000.nii.gz"   # Update this path
    test_label: "instancesTs/high_c2.nii.gz"   # Optional: for evaluation
    test_resolution: [16, 16, 16]               # Typical resolution (nm)

    # test_image: "datasets/lucchi++/test_im.h5"   # Update this path
    # test_label: "datasets/lucchi++/test_mito.h5"   # Optional: for evaluation
    # test_resolution: [5, 5, 5]               # Typical resolution (nm)

    output_path: outputs/mednext_mitoverse/results/
    output_name: "predictions.nii.gz"

    image_transform:
      normalize: "divide-255"            # Divide by 255 (matches BANIS normalization)


  # MONAI SlidingWindowInferer parameters
  sliding_window:
    window_size: [128, 128, 128]     # Match training patch size
    overlap: 0.25                    # 25% overlap (reduced for memory)
    sw_batch_size: 1                 # Process 1 patch at a time (memory optimization)
    blending: gaussian               # Gaussian weighting at boundaries
    sigma_scale: 0.25
    padding_mode: reflect
    save_channels: [0, 1, 6]         # Save affinity x, y, and SDT only

  # Test-Time Augmentation
  test_time_augmentation:
    enabled: false                   # Disable for faster inference
    flip_axes: null
    # Channel activations for affinity prediction
    # After save_channels [0,1,6] filtering, new indices are:
    #   Original ch 0 → New ch 0 (affinity x)
    #   Original ch 1 → New ch 1 (affinity y)
    #   Original ch 6 → New ch 2 (SDT)
    # Note: BANIS uses scale_sigmoid = sigmoid(0.2 * x) for numerical stability in fp16
    channel_activations:
      - [0, 2, sigmoid]        # Affinities x,y with scaled sigmoid
      - [2, 3, tanh]                 # SDT (new ch 2)
    select_channel: all
    ensemble_mode: mean
    apply_mask: false

  # Save intermediate predictions
  save_prediction:
    enabled: true
    intensity_scale: 1.0             # Keep original scale for float16
    intensity_dtype: float16         # Save as float16 for precision

  # BANIS two-stage watershed post-processing for instance segmentation
  # Uses affinity channels (foreground) + SDT channel (seeds/distance)
  #
  # This matches the original BANIS inference script with:
  #   --apply_watershed --skeleton_threshold 0 --binary_threshold 0.5
  #
  # After save_channels [0,1,6] filtering, predictions are 3-channel:
  #   - ch 0: affinity x after sigmoid
  #   - ch 1: affinity y after sigmoid
  #   - ch 2: SDT after tanh
  decoding:
    - name: decode_binary_contour_distance_watershed
      kwargs:
        # Channel indices after save_channels filtering (3 channels total)
        binary_channels: [0, 1]          # Average affinity x and y for binary mask
        contour_channels: null           # No contour channels
        distance_channels: [2]           # SDT channel for distance
        # BANIS-style parameters (matching original inference script)
        binary_threshold: [1, 0.5]       # seed_thr=1, fg_thr=0.5 for affinities
        contour_threshold: null          # Disable contour (BANIS doesn't use contour)
        distance_threshold: [0.0, -1.0]  # seed_thr=0, fg_thr=-1 (SDT seeds where > 0)
        min_instance_size: 100           # Minimum voxels per instance
        min_seed_size: 8                 # Minimum seed size
        remove_small_mode: background    # Replace small objects with background
        prediction_scale: 1              # Predictions are in 0-1 range after activation

  # Evaluation metrics
  evaluation:
    enabled: true
    metrics: [instance_matching]     # VOI: Variation of Information
