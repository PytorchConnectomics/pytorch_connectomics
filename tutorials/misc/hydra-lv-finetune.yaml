# Hydra Large Vesicle Fine-tuning V2 - IMPROVED Configuration
#
# This config implements the improvements from HYDRA_LV_FINETUNE_IMPROVEMENT_PLAN.md
# to fix poor performance (ARE=0.471, Recall=0.389)
#
# KEY CHANGES FROM V1:
# ==================
# 1. iter_num_per_epoch: 1280 → 5000 (4x more training)
# 2. deep_supervision: false → true (CRITICAL for small objects)
# 3. augmentation: "some" → "all" (maximum diversity)
# 4. lr: 1e-4 → 3e-4 (3x higher, less conservative)
# 5. loss_weights: rebalanced to emphasize binary/boundary
# 6. max_epochs: 500 → 1000 (more training time)
#
# EXPECTED RESULTS:
# ================
# - Adapted Rand Error: < 0.1 (was: 0.471)
# - Precision: > 0.95 (was: 0.828)
# - Recall: > 0.95 (was: 0.389)
# - Total Loss: < 0.2 (was: 0.555)
#
# ============================================================================

experiment_name: hydra-lv_finetune_rsunet_v2
description: Improved fine-tuning with deep supervision and aggressive training

# System
system:
  training:
    num_gpus: 4
    num_workers: 2
    batch_size: 4  # CHANGED FROM 8 (better gradient updates)
  inference:
    num_gpus: 1
    num_workers: 1
    batch_size: 1
  seed: 42

# Model Configuration
model:
  architecture: rsunet

  # PRETRAINED MODEL LOADING
  external_weights_path: "outputs/hydra-lv_rsunet/20251203_012234/checkpoints/last.ckpt"
  external_weights_key_prefix: "model."

  # Model architecture
  input_size: [32, 128, 128]
  output_size: [32, 128, 128]
  in_channels: 1
  out_channels: 3  # binary, boundary, distance

  # Architecture settings
  filters: [32, 64, 128, 256, 512]
  strides: [2, 2, 2]
  num_res_units: 2
  kernel_size: 3
  norm: batch
  dropout: 0.1
  rsunet_norm: batch

  # ⭐⭐⭐ CRITICAL FIX: Enable deep supervision for small objects
  deep_supervision: true  # CHANGED FROM: false

  # Loss configuration - REBALANCED to emphasize binary and boundary
  loss_functions: [DiceLoss, WeightedBCEWithLogitsLoss, DiceLoss, TverskyLoss, SmoothL1Loss]
  loss_weights: [2.0, 2.0, 1.0, 1.0, 0.5]  # CHANGED FROM: [1.0, 1.0, 1.0, 1.0, 1.0]
  # Breakdown:
  #   - DiceLoss (binary): 2.0 ← emphasize object detection
  #   - WeightedBCE (binary): 2.0 ← emphasize object detection
  #   - DiceLoss (boundary): 1.0
  #   - TverskyLoss (boundary): 1.0
  #   - SmoothL1Loss (EDT): 0.5 ← reduce (already converged)

  loss_kwargs:
    - {sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}
    - {reduction: mean}
    - {sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}
    - {sigmoid: true, alpha: 0.6, beta: 0.4, smooth_nr: 1e-5, smooth_dr: 1e-5}  # More recall-focused (was 0.7/0.3)
    - {beta: 0.1, reduction: mean, tanh: true}

  loss_balancing:
    strategy: uncertainty  # Keep uncertainty weighting

  loss_terms:
    - name: label_loss_0
      loss_index: 0
      pred_slice: [0, 1]
      target_slice: [0, 1]
      task_name: label
    - name: label_loss_1
      loss_index: 1
      pred_slice: [0, 1]
      target_slice: [0, 1]
      task_name: label
    - name: boundary_loss_2
      loss_index: 2
      pred_slice: [1, 2]
      target_slice: [1, 2]
      task_name: boundary
    - name: boundary_loss_3
      loss_index: 3
      pred_slice: [1, 2]
      target_slice: [1, 2]
      task_name: boundary
    - name: edt_loss_4
      loss_index: 4
      pred_slice: [2, 3]
      target_slice: [2, 3]
      task_name: edt
data:
  # Training data
  train_image: "datasets/bouton-lv/train/vol0_im.h5"
  train_label: "datasets/bouton-lv/train/vol0_lv.h5"
  train_mask: "datasets/bouton-lv/train/vol0_mask.h5"
  train_resolution: [30, 8, 8]

  use_preloaded_cache: true
  cache_rate: 0.0
  persistent_workers: false

  # Patch configuration
  patch_size: [32, 128, 128]
  pad_size: [8, 24, 24]
  pad_mode: reflect

  # ⭐⭐⭐ CRITICAL FIX: 4x more iterations per epoch
  iter_num_per_epoch: 5000  # CHANGED FROM: 1280 (severe undertraining)

  # Image normalization
  image_transform:
    normalize: "0-1"
    clip_percentile_low: 0.0
    clip_percentile_high: 1.0

  # Label transformation
  label_transform:
    targets:
      - name: binary
      - name: instance_boundary
        kwargs:
          thickness: 1
          edge_mode: "all"
          mode: "2d"
      - name: instance_edt
        kwargs:
          mode: "3d"
          quantize: false

  # ⭐⭐⭐ CRITICAL FIX: Maximum augmentation for small dataset
  augmentation:
    preset: "all"  # CHANGED FROM: "some"

    flip:
      enabled: true
      prob: 0.8  # INCREASED FROM: 0.5
      spatial_axis: [0, 1, 2]

    rotate:
      enabled: true
      prob: 0.8  # INCREASED FROM: 0.5
      spatial_axes: [1, 2]

    affine:
      enabled: true
      prob: 0.5  # INCREASED FROM: 0.3
      rotate_range: [0.1, 0.1, 0.1]
      scale_range: [0.1, 0.1, 0.1]  # INCREASED FROM: 0.05
      shear_range: [0.05, 0.05, 0.05]

    intensity:
      enabled: true
      gaussian_noise_prob: 0.5  # INCREASED FROM: 0.2
      gaussian_noise_std: 0.03
      shift_intensity_prob: 0.6  # INCREASED FROM: 0.4
      shift_intensity_offset: 0.1
      contrast_prob: 0.6  # INCREASED FROM: 0.4
      contrast_range: [0.8, 1.2]

    misalignment:
      enabled: true
      prob: 0.4
      displacement: 8
      rotate_ratio: 0.3

    missing_section:
      enabled: true
      prob: 0.3
      num_sections: 2

    motion_blur:
      enabled: true
      prob: 0.3
      sections: 2
      kernel_size: 9

# Optimization - IMPROVED settings
optimization:
  max_epochs: 1000  # INCREASED FROM: 500 (need more time to converge)
  gradient_clip_val: 0.5
  accumulate_grad_batches: 2
  precision: "bf16-mixed"

  optimizer:
    name: AdamW
    # ⭐⭐ HIGHER LR: More aggressive for small dataset
    lr: 0.0003  # INCREASED FROM: 0.0001 (3x higher)
    weight_decay: 0.001  # DECREASED FROM: 0.01 (less regularization)
    betas: [0.9, 0.999]
    eps: 1.0e-8

  # Scheduler - longer warmup, higher min_lr
  scheduler:
    name: warmupcosine
    warmup_epochs: 100  # INCREASED FROM: 50
    warmup_start_lr: 0.1
    min_lr: 1.0e-5  # INCREASED FROM: 1e-6 (stay useful longer)

  # EMA
  ema:
    enabled: true
    decay: 0.999
    warmup_steps: 1000
    validate_with_ema: true

# Monitoring
monitor:
  detect_anomaly: false
  logging:
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 10
      val_check_interval: 1.0
      benchmark: true

    images:
      enabled: true
      max_images: 2
      num_slices: 4
      log_every_n_epochs: 1
      channel_mode: all
      selected_channels:

  checkpoint:
    mode: min
    save_top_k: 3
    save_last: true
    save_every_n_epochs: 10
    dirpath: outputs/hydra-lv_rsunet_finetune_v2/checkpoints/
    use_timestamp: true

  early_stopping:
    enabled: true
    monitor: train_loss_total_epoch
    patience: 100  # INCREASED FROM: 50 (more time since iter_num increased)
    mode: min
    min_delta: 1.0e-5  # DECREASED FROM: 1e-4 (more sensitive)
    check_finite: true
    threshold: 0.02
    divergence_threshold: 100.0

# Inference
inference:
  sliding_window:
    window_size: [32, 128, 128]
    sw_batch_size: 1
    overlap: 0.25
    blending: gaussian
    sigma_scale: 0.25
    padding_mode: replicate

  # ⭐ IMPROVEMENT: Enable TTA for better evaluation
  test_time_augmentation:
    flip_axes: [[0], [1], [2], [0, 1], [0, 2], [1, 2]]  # CHANGED FROM: null
    rotation90_axes: [[1, 2]]  # CHANGED FROM: null
    select_channel: all
    channel_activations:
      - [0, 1, sigmoid]
      - [1, 2, sigmoid]
      - [2, 3, tanh]
    ensemble_mode: mean
    apply_mask: true

  save_prediction:
    enabled: true
    intensity_scale: -1

# Test configuration
test:
  data:
    test_image: "datasets/bouton-lv/train/vol0_im.h5"
    test_label: "datasets/bouton-lv/train/vol0_lv.h5"
    test_mask: "datasets/bouton-lv/train/vol0_mask.h5"
    test_resolution: [30, 8, 8]
    image_transform:
      normalize: "0-1"
      clip_percentile_low: 0.0
      clip_percentile_high: 1.0

  # ⭐ IMPROVEMENT: More sensitive thresholds for better recall
  decoding:
    - name: decode_instance_binary_contour_distance
      kwargs:
        binary_threshold: [0.3, 0.7]  # CHANGED FROM: [0.5, 0.7] (lower = more sensitive)
        contour_threshold: [0.5, 13.1]  # CHANGED FROM: [0.8, 13.1] (lower = allow weaker boundaries)
        distance_threshold: [0.5, -0.5]
        min_seed_size: 8  # CHANGED FROM: 4 (filter more noise)

  evaluation:
    enabled: true
    metrics: [adapted_rand]  # Now includes precision + recall!

# ============================================================================
# TRAINING INSTRUCTIONS:
# ============================================================================
#
# 1. Train from scratch:
#    python scripts/main.py --config tutorials/hydra-lv-finetune-v2.yaml
#
# 2. Monitor progress:
#    tensorboard --logdir outputs/hydra-lv_rsunet_finetune_v2/
#
# 3. Expected timeline:
#    - Epoch 50: Loss should be < 0.8
#    - Epoch 200: Loss should be < 0.3
#    - Epoch 500: Loss should be < 0.2
#    - Epoch 1000: Loss should be < 0.1 (target)
#
# 4. Success criteria:
#    - Loss converges to < 0.2
#    - Training visualizations show good predictions
#    - Test ARE < 0.1, Recall > 0.95
#
# 5. If still not working by epoch 500:
#    - Try removing pretrained weights (set external_weights_path: null)
#    - Try smaller architecture (filters: [16, 32, 64, 128, 256])
#    - Try MedNeXt architecture (see improvement plan)
#
# ============================================================================
