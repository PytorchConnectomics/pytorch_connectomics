



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Neuron Segmentation &mdash; connectomics latest documentation</title>
  

  
  
  
  

  

  
  
  

  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/pytc-theme.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/js@alpha" type="text/css" />
  <link rel="stylesheet" href="../_static/css/readthedocs-doc-embed.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Mitochondria Segmentation" href="mito.html" />
  <link rel="prev" title="FAQ" href="../notes/faq.html" /> 

    <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">

  <link rel="stylesheet" href="text.css" type="text/css" />

  <!-- at the end of the HEAD -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@alpha" />
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">

    <a class="header-logo" href="../index.html" aria-label="PyTC"></a>

    <div class="header-container">

      <div class="main-menu">
        <ul>
          <li>
            <a href="../notes/installation.html">Get Started</a>
          </li>
          <li>
            <a href="#">Tutorials</a>
          </li>
          <li>
            <a href="../index.html">Docs</a>
          </li>
          <li>
            <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master">GitHub</a>
          </li>
          <li>
            <a href="../about/team.html">About Us</a>
          </li>

        </ul>
      </div>

      <!-- <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a> -->
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          <div class="version">
            latest
          </div>
          
          

          <div id="docsearch"></div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/config.html">Configuration System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/dataloading.html">Data Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/migration.html">Migration Guide (v1.0 → v2.0)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Neuron Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mito.html">Mitochondria Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="synapse.html">Synapse Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="artifact.html">Artifacts Detection (Draft)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">External Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../external/neuroglancer.html">Neuroglancer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/lightning.html">Lightning Module API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/model.html">connectomics.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/data.html">connectomics.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/utils.html">connectomics.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../about/team.html">About Us</a></li>
</ul>

        
        
      </div>
    </div>

    


    

    <!-- 
    
    <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
      <span class="rst-current-version" data-toggle="rst-current-version">
        <span class="fa fa-book"> Read the Docs</span>
        v: latest
        <span class="fa fa-caret-down"></span>
      </span>
      <div class="rst-other-versions">
        <dl>
          <dt>Versions</dt>
          
          <dd><a href="#">latest</a></dd>
          
        </dl>
        <dl>
          <dt>Downloads</dt>
          <dd><a href="https://github.com/zudi-lin/pytorch_connectomics/">PDF</a>
          </dd>
          <dd><a href="https://github.com/zudi-lin/pytorch_connectomics/">HTML</a></dd>
        </dl>
        <dl>
          <dt>On Github</dt>
          <dd><a href="https://github.com/zudi-lin/pytorch_connectomics">Home</a></dd>
          <dd><a href="https://github.com/zudi-lin/pytorch_connectomics/">Docs</a></dd>
        </dl>
      </div>
    </div>
    
     -->

  </nav>


  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Neuron Segmentation</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/neuron.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="neuron-segmentation">
<h1>Neuron Segmentation<a class="headerlink" href="#neuron-segmentation" title="Link to this heading">¶</a></h1>
<p>This tutorial provides step-by-step guidance for neuron segmentation with SENMI3D benchmark datasets.
Dense neuron segmentation in electronic microscopy (EM) images belongs to the category of <strong>instance segmentation</strong>.
The methodology is to first predict the affinity map (the connectivity of each pixel to neighboring pixels)
with an encoder-decoder ConvNets and then generate the segmentation map using a standard
segmentation algorithm (<em>e.g.</em>, watershed).</p>
<p>The evaluation of segmentation results is based on the <a class="reference external" href="https://en.wikipedia.org/wiki/Rand_index">Rand Index</a>
and <a class="reference external" href="https://en.wikipedia.org/wiki/Variation_of_information">Variation of Information</a>.</p>
<blockquote>
<div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Before running neuron segmentation, please take a look at the <a class="reference external" href="https://github.com/zudi-lin/pytorch_connectomics/tree/master/notebooks">notebooks</a> to get familiar with the datasets and available utility functions in this package.</p>
</div>
</div></blockquote>
<p>The main script to run the training and inference is <code class="docutils literal notranslate"><span class="pre">pytorch_connectomics/scripts/main.py</span></code>.
The pytorch target affinity generation is <code class="xref py py-class docutils literal notranslate"><span class="pre">connectomics.data.dataset.VolumeDataset</span></code>.</p>
<section id="neighboring-affinity-learning">
<h2>Neighboring affinity learning<a class="headerlink" href="#neighboring-affinity-learning" title="Link to this heading">¶</a></h2>
<p>The affinity value between two neighboring pixels (voxels) is 1 if they belong to the same instance and 0 if
they belong to different instances or at least one of them is a background pixel (voxel). An affinity map can
be regarded as a more informative version of boundary map as it contains the affinity to two directions in 2D inputs and three directions (<cite>z</cite>, <cite>y</cite> and <cite>x</cite> axes) in 3D inputs.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/snemi_affinity.png"><img alt="../_images/snemi_affinity.png" src="../_images/snemi_affinity.png" style="width: 800px;" />
</a>
</figure>
<p>The figure above shows examples of EM images, segmentation and affinity map from the SNEMI3D dataset. Since the
3D affinity map has 3 channels, we can visualize them as RGB images.</p>
<blockquote>
<div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A notebook for visualizing results is provided for users in the <a class="reference external" href="https://github.com/zudi-lin/pytorch_connectomics/tree/master/notebooks/tutorial_benchmarks/snemi_benchmark.ipynb">Github repository</a>. Users are able to download this notebook and produce evaluation results using a pretrained benchmark. Due to incompatiability with Colab and neuroglancer, it is recommended that users utilize a personal computer/HPC with at least 12GB of video RAM.</p>
</div>
</div></blockquote>
<section id="get-the-data">
<h3>1 - Get the data<a class="headerlink" href="#get-the-data" title="Link to this heading">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>wget http://rhoana.rc.fas.harvard.edu/dataset/snemi.zip
</pre></div>
</div>
<blockquote>
<div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>As of April 9, 2024, unzipping the above folder will create an <code class="docutils literal notranslate"><span class="pre">image</span></code> and <code class="docutils literal notranslate"><span class="pre">seg</span></code> folder. It is recommended that these two folders be placed under datasets/SNEMI3D or that <code class="docutils literal notranslate"><span class="pre">configs/SNEMI/SNEMI-Base.yaml</span></code> be changed to point to the appropriate dataset paths.</p>
</div>
</div></blockquote>
<p>For description of the SNEMI dataset please check <a class="reference external" href="https://vcg.github.io/newbie-wiki/build/html/data/data_em.html">this page</a>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since for a region with dense masks, most affinity values are 1, in practice, we usually widen the instance border (erode the instance mask) to deal with the class imbalance problem and let the model make more conservative predictions to prevent merge error. This is done by setting <code class="docutils literal notranslate"><span class="pre">MODEL.LABEL_EROSION</span> <span class="pre">=</span> <span class="pre">1</span></code>.</p>
</div>
</div></blockquote>
</section>
<section id="run-training">
<h3>2 - Run training<a class="headerlink" href="#run-training" title="Link to this heading">¶</a></h3>
<p>Provide the <strong>YAML</strong> configuration files to run training:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>source activate py3_torch
python -u scripts/main.py \
--config-base configs/SNEMI/SNEMI-Base.yaml \
--config-file configs/SNEMI/SNEMI-Affinity-UNet.yaml
</pre></div>
</div>
<p>Or if using multiple GPUs for higher performance:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u -m torch.distributed.run \
--nproc_per_node=2 --master_port=1234 scripts/main.py --distributed \
--config-base configs/SNEMI/SNEMI-Base_multiGPU.yaml \
--config-file configs/SNEMI/SNEMI-Affinity-UNet.yaml
</pre></div>
</div>
<p>The configuration files for training can be found in <code class="docutils literal notranslate"><span class="pre">configs/SNEMI/</span></code>.
We usually create a <code class="docutils literal notranslate"><span class="pre">datasets/</span></code> folder under <code class="docutils literal notranslate"><span class="pre">pytorch_connectomics</span></code> and put the SNEMI dataset there.
Please modify the following options according to your system configuration and data storage:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">IMAGE_NAME</span></code>: name of the 3D image file (HDF5 or TIFF)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LABEL_NAME</span></code>: name of the 3D label file (HDF5 or TIFF)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INPUT_PATH</span></code>: directory path to both input files above</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OUTPUT_PATH</span></code>: path to save outputs (checkpoints and Tensorboard events)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NUM_GPUS</span></code>: number of GPUs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NUM_CPUS</span></code>: number of CPU cores (for data loading)</p>
<blockquote>
<div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By default, we use multi-process distributed training with one GPU per process (and multiple CPUs for data loading). The model is wrapped with <a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">DistributedDataParallel</a> (DDP). For more benefits of DDP, check <a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">this tutorial</a>. Please note that official synchronized batch normalization (SyncBN) in PyTorch is only supported with DDP.</p>
</div>
</div></blockquote>
</li>
</ul>
<p>We also support <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html">data parallel</a> (DP) training.
If the training command above does not work for your system, please use:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u scripts/main.py \
--config-base configs/SNEMI/SNEMI-Base.yaml \
--config-file configs/SNEMI/SNEMI-Affinity-UNet.yaml
</pre></div>
</div>
<p>DDP training is our default settings because features like automatic mixed-precision training and synchronized batch
normalization are better supported for DDP. Besides, DP usually has an imbalanced GPU memory usage.</p>
</section>
<section id="run-training-with-pretrained-model-optional">
<h3>3 - Run training with pretrained model (<em>optional</em>)<a class="headerlink" href="#run-training-with-pretrained-model-optional" title="Link to this heading">¶</a></h3>
<p>(<em>Optional</em>) To run training starting from pretrained weights, add a checkpoint file:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u -m torch.distributed.run \
--nproc_per_node=2 --master_port=1234 scripts/main.py --distributed \
--config-base configs/SNEMI/SNEMI-Base.yaml \
--config-file configs/SNEMI/SNEMI-Affinity-UNet.yaml \
--checkpoint /path/to/checkpoint/checkpoint_xxxxx.pth.tar
</pre></div>
</div>
</section>
<section id="visualize-the-training-progress">
<h3>4 - Visualize the training progress<a class="headerlink" href="#visualize-the-training-progress" title="Link to this heading">¶</a></h3>
<p>We use Tensorboard to visualize the training process. Specify <code class="docutils literal notranslate"><span class="pre">--logdir</span></code> with your own experiment directory, which can be different
from the default one.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir outputs/SNEMI_UNet/
</pre></div>
</div>
<p>To visualize the training process and generate a <strong>public link</strong> to share the results with collaborators, we
use <a class="reference external" href="https://tensorboard.dev/">tensorboard dev</a>. Similar to local visualization, we specify <code class="docutils literal notranslate"><span class="pre">--logdir</span></code> with the experiment
directory (which can be different from the default one).</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tensorboard dev upload --logdir outputs/SNEMI_UNet/
</pre></div>
</div>
<p>Please refer this <a class="reference external" href="https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tbdev_getting_started.ipynb#scrollTo=oKW8V5chyx6e">example</a> Google Colab
notebook for a step-by-step tutorial. Please also note that Tensorboard Dev <a class="reference external" href="https://github.com/tensorflow/tensorboard/issues/3585/">does not suppport</a> images
in the visualization with public link as of 12 October, 2021.</p>
</section>
<section id="inference-of-affinity-map">
<h3>5 - Inference of affinity map<a class="headerlink" href="#inference-of-affinity-map" title="Link to this heading">¶</a></h3>
<p>Run inference on image volumes (add <code class="docutils literal notranslate"><span class="pre">--inference</span></code>). During inference the model can use larger batch sizes or take bigger inputs.
Test-time augmentation is also applied by default. We do not use distributed data-parallel during inference as the back-propagation
is not needed.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python -u scripts/main.py --config-base configs/SNEMI/SNEMI-Base.yaml \
--config-file configs/SNEMI/SNEMI-Affinity-UNet.yaml --inference \
--checkpoint outputs/SNEMI_UNet/checkpoint_100000.pth.tar
</pre></div>
</div>
</section>
<section id="get-segmentation">
<h3>6 - Get segmentation<a class="headerlink" href="#get-segmentation" title="Link to this heading">¶</a></h3>
<p>The last step is to generate segmentation (with external post processing packages) and run
evaluation. First download the <code class="docutils literal notranslate"><span class="pre">waterz</span></code> package <a class="reference external" href="https://github.com/zudi-lin/waterz">here</a>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/zudi-lin/waterz.git
cd waterz
pip install --editable .
pip install waterz
</pre></div>
</div>
<p>Follow the instructions on the repository to install the <code class="docutils literal notranslate"><span class="pre">waterz</span></code> package. We will use the <code class="docutils literal notranslate"><span class="pre">waterz.waterz</span></code> API to generate segmentation from the affinity maps. The API takes in as arguments.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">affinities</span></code>. This is the affinity map generated by our model in the previous step. The values in the affinity map is expected to be between <code class="docutils literal notranslate"><span class="pre">aff_threshold[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">aff_threshold[1]</span></code>. The affinity values should be float between 0 and 1 but the affinity map prediicted by the model are between 0 and 255 in uint8 (to save storage). Hence before using the affinity map we need to <em>divide it by 255</em>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aff_thresholds</span></code>. The values in the affinity maps will be constrained to lie between these thresholds. Recommended values are <code class="docutils literal notranslate"><span class="pre">[0.05,0.995]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">seg_thresholds</span></code>. This is an array of segmentation threshold values. Recommended values are <code class="docutils literal notranslate"><span class="pre">[0.1,0.3,0.6]</span></code>. The API will produce a segmentation volume for each segmentation threshold in the array.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">merge_function</span></code>. The function that will be used while merging the nodes of the region adjacency graph. Recommended value for this parameter is  <code class="docutils literal notranslate"><span class="pre">&quot;aff50_his256&quot;</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">seg_gt</span></code>. This is the ground-truth segmentation used for evaluating the segmentation result. If ground truth is not available, this parameter is supposed to be <code class="docutils literal notranslate"><span class="pre">None</span></code>. If the ground truth is available, the API prints the <em>Rand</em> and <em>VOI</em> scores.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">waterz</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># affinities is a [3, depth, height, width] numpy array of uint8 if predicted by PyTC</span>
<span class="n">affinities</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># model prediction</span>

<span class="n">affinities</span> <span class="o">=</span> <span class="n">affinities</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="c1"># The affinity values in the model prediction are in the interval [0,255] and the affinity thresholds provided constraint them</span>
<span class="c1"># in the interval [0.05,0.995] hence we divide it by 255 in order to scale it.</span>

<span class="c1"># evaluation: vi/rand</span>
<span class="n">seg_gt</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># segmentation ground truth. If available, the prediction is evaluated against this ground truth and Rand and VI scores are produced.</span>

<span class="n">aff_thresholds</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.995</span><span class="p">]</span>
<span class="n">seg_thresholds</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>

<span class="n">seg</span> <span class="o">=</span> <span class="n">waterz</span><span class="o">.</span><span class="n">waterz</span><span class="p">(</span><span class="n">affinities</span><span class="p">,</span> <span class="n">seg_thresholds</span><span class="p">,</span> <span class="n">merge_function</span><span class="o">=</span><span class="s1">&#39;aff50_his256&#39;</span><span class="p">,</span>
          <span class="n">aff_threshold</span><span class="o">=</span><span class="n">aff_thresholds</span><span class="p">,</span> <span class="n">gt</span><span class="o">=</span><span class="n">seg_gt</span><span class="p">)</span>

<span class="c1"># seg will be an array of shape [3,depth,height,width]. Since there are 3 segmentation thresholds, we get a result of shape</span>
<span class="c1"># [depth,height,width] for each threshold.</span>
</pre></div>
</div>
<p>Optionally, the <code class="docutils literal notranslate"><span class="pre">zwatershed</span></code> package can also be used to process the affinity map into
segmentation. See details <a class="reference external" href="https://github.com/zudi-lin/zwatershed">here</a>.</p>
</section>
</section>
<section id="long-range-affinity-learning">
<h2>Long-range affinity learning<a class="headerlink" href="#long-range-affinity-learning" title="Link to this heading">¶</a></h2>
<p>ToDo</p>
</section>
<section id="semi-supervised-affinity-learning">
<h2>Semi-supervised affinity learning<a class="headerlink" href="#semi-supervised-affinity-learning" title="Link to this heading">¶</a></h2>
<p>ToDo</p>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="mito.html" class="btn btn-neutral float-right" title="Mitochondria Segmentation" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-orange.svg"
        class="next-page"></a>
    
    
    <a href="../notes/faq.html" class="btn btn-neutral" title="FAQ" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
    
  </div>
  

  

  <hr>

  

  <div role="contentinfo">
    <p>
      &copy; Copyright 2019-2025, PyTorch Connectomics Contributors.

    </p>
  </div>
  
  <div style="margin-bottom:1cm">
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Neuron Segmentation</a><ul>
<li><a class="reference internal" href="#neighboring-affinity-learning">Neighboring affinity learning</a><ul>
<li><a class="reference internal" href="#get-the-data">1 - Get the data</a></li>
<li><a class="reference internal" href="#run-training">2 - Run training</a></li>
<li><a class="reference internal" href="#run-training-with-pretrained-model-optional">3 - Run training with pretrained model (<em>optional</em>)</a></li>
<li><a class="reference internal" href="#visualize-the-training-progress">4 - Visualize the training progress</a></li>
<li><a class="reference internal" href="#inference-of-affinity-map">5 - Inference of affinity map</a></li>
<li><a class="reference internal" href="#get-segmentation">6 - Get segmentation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#long-range-affinity-learning">Long-range affinity learning</a></li>
<li><a class="reference internal" href="#semi-supervised-affinity-learning">Semi-supervised affinity learning</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script src="../_static/documentation_options.js?v=f4332903"></script>
  <script src="../_static/doctools.js?v=9bcbadda"></script>
  <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <!-- <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Visual Computing Group</h2>
          <p>Visual computing group (VCG) led by Prof. Hanspeter Pfister at Harvard University</p>
          <a class="with-right-arrow" href="https://vcg.seas.harvard.edu/">View VCG</a>
        </div>
        <div class="col-md-4 text-center">
          <h2>Lichtman Lab</h2>
          <p>Neuroscience research lab led by Prof. Jeff Lichtman at Harvard University</p>
          <a class="with-right-arrow" href="https://lichtmanlab.fas.harvard.edu">View Lichtman Lab</a>
        </div>
        <div class="col-md-4 text-center">
          <h2>PyTorch</h2>
          <p>An open source machine learning framework</p>
          <a class="with-right-arrow" href="https://pytorch.org/">View PyTorch</a>
        </div>
      </div>
    </div> -->
  </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->
  <!--
  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>
    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>
          <li>
            <a href="#">Features</a>
          </li>
          <li>
            <a href="#">Ecosystem</a>
          </li>
          <li>
            <a href="">Blog</a>
          </li>
          <li>
            <a href="https://zudi-lin.github.io/pytorch_connectomics/build/html/tutorials/snemi.html">Tutorials</a>
          </li>
          <li>
            <a href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html">Docs</a>
          </li>
          <li>
            <a href="">Resources</a>
          </li>
          <li>
            <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  -->
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    var collapsedSections = ['Notes']
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- at the end of the BODY -->
  <script src="https://cdn.jsdelivr.net/npm/@docsearch/js@alpha"></script>
  <script>
    /* global docsearch */
    docsearch({
      container: "#docsearch",
      apiKey: "f072ddc06d4d2d86f6b26fb6f12a4699",
      indexName: "readthedocs",
      placeholder: "Search PyTorch Connectomics",
    });
  </script>

</body>

</html>